% !TEX root = doc.tex
% Copyright (c) 2016 The ALF project.
% This is a part of the ALF project documentation.
% The ALF project documentation by the ALF contributors is licensed
% under a Creative Commons Attribution-ShareAlike 4.0 International License.
% For the licensing details of the documentation see license.CCBYSA.
%------------------------------------------------------------
\subsection{MC sampling}
\label{sec:caveats}
%------------------------------------------------------------
%
The default updating scheme consists of local moves which changes (upon acceptance) only one single entry of $L_{\mathrm{Trotter}}(M_I+M_V)$ spins (see Sec. \ref{sec:updating}). It is obvious that a single spin flip does not generate a independent configuration $C$. Hence it will require at least as many local spin flips as there are spins in the configuration space. This is however only the lower bound as there can be a region in the spin space where the fields are correlated and it requires a larger or even global move to significantly change the configuration to an independent one. One might imagine a ferromagnet due to spontaneous symmetry breaking. All spins are parallel aligned and, let' say, point upwards. The configuration of only down spins is equally justified, but rotating one to the other requires a global operation. Flipping the spins individually one after another generates intermediate states of relative high energy which corresponds to a low probability in the QMC algorithm.

These considerations lead to the definition of the auto-correlation time $T_\mathrm{auto}$ that characterizes the required time scale to generate an independent configuration or values $\langle\langle\hat{O}\rangle\rangle_C$ for the Observable $O$.

This has several consequences for the Monte Carlo simulation:
\begin{itemize}
	\item First of all, we start from a randomly chosen field configuration such that one has to invest at least one $T_\mathrm{auto}$ to generate relevant configurations before reliable measurements are possible. This phase of the simulation is known as the warm-up. In order to keep the code as flexible as possible (different simulations might have different auto-correlation times), measurements are taken from the very beginning. Instead we provide the parameter \path{n_skip} for the analysis to ignore the first \path{n_skip} bins.
	\item Secondly, our implementation averages over a given amount of measurements (details see ...) before storing the results, known as one bin, on the disk. The following error analysis requires independent bins to generate reliable confidence estimates. If bins are to small (averaged over a period shorter then $T_\mathrm{auto}$), the error bars are then typically underestimated. Most of the time, the auto-correlation time is unknown before the simulation is started, sometime, the compute cluster does not allow single runs long enough to generate appropriately sized bins. Therefore we provide the \path{N_rebin} parameter that specifies how many bins are combined into a new bin during the error analysis. In general, one should check, that a further increase of the bin size does not change the error estimate any more such that the results have converged.
	
	REMARK: The \path{N_rebin} variable can be used to control a second issue. The distribution of the Monte Carlo estimates $\langle\langle\hat{O}\rangle\rangle_C$ are unknown. The result in the form $(\mathrm{best}\pm \mathrm{error})$ assumes a Gaussian distribution. Luckily, every original distribution with a finite variance turns into a Gaussian one, once if is folded often enough (central limit theorem). Due to the internal averaging (folding) within on bin, many observables are already quite Gaussian. Otherwise one can increase \path{N_rebin} further, even if the bins are independent already~\cite{Bercx17}.
	\item The third caveat concerns time-resolved correlation functions. Even if the configurations are independent, the fields within the configuration are still correlated. Hence, the data for $S_{\alpha,\beta}(\vec{k},\tau)$ (see Sec.~\ref{sec:obs}; Eqn.~\ref{eqn:s}) and $S_{\alpha,\beta}(\vec{k},\tau+\Delta\tau)$ are also correlated. Setting the switch \path{N_Cov = 1} triggers the calculation of the covariance matrix in addition to the usual error analysis. The covariance is defined by
	\begin{equation}
		Cov_{\tau \tau'}=\frac{1}{N_{Bins}}\left\langle\left(S_{\alpha,\beta}(\vec{k},\tau)-\langle S_{\alpha,\beta}(\vec{k},\tau)\rangle\right)\left(S_{\alpha,\beta}(\vec{k},\tau')-\langle S_{\alpha,\beta}(\vec{k},\tau')\rangle\right)\right\rangle\,.
	\end{equation}
	An example where this information is the extraction of energy scales by fitting the tail around $\frac{\beta}{2}$ that would otherwise underestimate the uncertainty.
\end{itemize}
