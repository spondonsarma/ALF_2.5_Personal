% Copyright (c) 2016 2017 The ALF project.
% This is a part of the ALF project documentation.
% The ALF project documentation by the ALF contributors is licensed
% under a Creative Commons Attribution-ShareAlike 4.0 International License.
% For the licensing details of the documentation see license.CCBYSA.

% !TEX root = doc.tex


In this section we describe the steps how to compile and run the code, as well as how to perform the error analysis of the data.
%
%-------------------------------------------------------------------------------------
\subsection{Compilation}
\label{sec:compilation}
%-------------------------------------------------------------------------------------
%
The environment variables and the directives to compile the code are set in the following makefile \texttt{Makefile}:  \red{[NOW A GOOD PART OF THIS IS IN THE SCRIPT configureHPC.sh -- Update]}
\begin{lstlisting}[style=bash]

# -DMPI selects MPI.
# -DTEMPERING selects tempering mode.  MPI has to be switched on.
# -DSTAB1   Alternative stabilization, using the singular value decomposition.
# -DSTAB2   Alternative stabilization, lapack QR with  manual pivoting.
#           Packed form of QR factorization is not used.
# -DSTAB3   Alternative stabilization, using QR  with pivoting.
#           Internally, scales larger and smaller one are distinguished.
# -DLOG     Alternative stabilization, using QR  with pivoting.
#           Internally, scales are stored on log axsis to allow larger beta and
#           larger and smaller have to be distinguished.
# (no flag) Default  stabilization, using lapack QR with pivoting. 
#           Packed form of QR factorization  is used. 
# -DQRREF   Enables reference lapack implementation of QR decomposition.
# Recommendation: just use the -DMPI flag if you want to run in parallel or 
#                 leave it empty for serial jobs.  
#                 The default stabilization, no flag, is generically the best. 
#                 Consider using -DLOG if you run into overflows
PROGRAMCONFIGURATION = -DMPI 
PROGRAMCONFIGURATION = 
f90 = gfortran
export f90
F90OPTFLAGS = -O3 -Wconversion  -fcheck=all
F90OPTFLAGS = -O3
export F90OPTFLAGS
F90USEFULFLAGS = -cpp -std=f2003
F90USEFULFLAGS = -cpp
export F90USEFULFLAGS
FL = -c ${F90OPTFLAGS} ${PROGRAMCONFIGURATION}
export FL
DIR = ${CURDIR}
export DIR
Libs = ${DIR}/Libraries/
export Libs
LIB_BLAS_LAPACK = -llapack -lblas
export LIB_BLAS_LAPACK

all: lib ana program

lib:
	cd Libraries && $(MAKE)
ana:
	cd Analysis && $(MAKE)
program:
	cd Prog && $(MAKE)


clean: cleanall
cleanall: cleanprog cleanlib cleanana
cleanprog:
	cd Prog && $(MAKE) clean
cleanlib:
	cd Libraries && $(MAKE) clean
cleanana:
	cd Analysis && $(MAKE) clean
help:
	@echo "The following are some of the valid targets of this Makefile"
	@echo "all, program, lib, ana, clean, cleanall, cleanprog, cleanlib,
	       cleanana"

\end{lstlisting}
In the above, the GNU Fortan compiler \texttt{gfortran} is set.\footnote{A known issue with the alternative Intel Fortran compiler \texttt{ifort} is the handling of automatic, temporary arrays 
which \texttt{ifort} allocates on the stack. For large system sizes and/or low temperatures this may lead to 
a runtime error. One solution is to demand allocation of arrays above a certain size on the heap instead of the stack. 
This is accomplished by the \texttt{ifort} compiler flag \texttt{-heap-arrays [n]} where \texttt{[n]} is the minimal size (in kilobytes, for example \texttt{n=1024}) of arrays 
that are allocated on the heap.}
We provide a set of options for compilation of the QMC code. The present options are \texttt{-DMPI}, \texttt{-DQRREF}, \texttt{-DSTAB1}, and \texttt{-DSTAB2}. 
They can be included in the string variable \texttt{PROGRAMCONFIGURATION} by the user, as shown above.
The program can be compiled and ran either in single-thread mode (default) or 
in multi-threading mode (define \texttt{-DMPI}) using the MPI standard for parallelization. The remaining three compiler options select a particular stabilization scheme for the matrix multiplications (see Sec.~\ref{sec:output_prec}).
To compile the libraries, the analysis routines and the QMC program at once, just execute the single command:
\begin{verbatim}
make
\end{verbatim}
To clean up all directories and remove the object files and executables, execute the command \texttt{make clean}. As can be seen in the above makefile, there exist also rules to compile/clean up the library, the analysis routines and the QMC program separately.  


A suite of tests for individual parts of the code (subroutines, functions, operations, etc.) is available at the directory \path{testsuite}. The tests can be run by executing the following sequence of commands (the script \path{configureHPC.sh} sets environment variables and is described in Sec.~\ref{sec:running}.):
\begin{lstlisting}[style=bash,morekeywords={make,cmake,ctest}]

source configureHPC.sh Devel serial
gfortran -v
make lib
make ana
make Examples
cd testsuite
cmake -E make_directory tests
cd tests
cmake -G "Unix Makefiles" -DCMAKE_Fortran_FLAGS_RELEASE=${F90OPTFLAGS} \
-DCMAKE_BUILD_TYPE=RELEASE ..
cmake --build . --target all --config Release
ctest -VV -O log.txt
\end{lstlisting}
which will output test results and total success rate.

%
%-------------------------------------------------------------------------------------
\subsection{Starting a simulation}
%-------------------------------------------------------------------------------------
%
To start a simulation from scratch, the following files have to be present: \texttt{parameters} and \texttt{seeds}. 
To run a single-thread simulation, for example by using the parameters of one of the  Hubbard models described in Sec.~\ref{sec:ex}, issue the command
\begin{verbatim}
./Prog/Examples.out
\end{verbatim}
To restart the code using an existing simulation as a starting point, first run the script \texttt{out\_to\_in.sh}, which copies outputted field configurations into input files.
%
%-------------------------------------------------------------------------------------
\subsection{Error analysis}\label{sec:analysis}
%-------------------------------------------------------------------------------------
%

\red{\texttt{./setenv.sh} does not exist anymore -- a lot to update here}

Note that the error analysis script requires the presence of the environment variable \path{DIR} which defines the path to the error analysis programs.
So before starting the error analysis, one has to make this variable available which is done by the script \path{setenv.sh}. The command is
\begin{verbatim}
source ./setenv.sh
\end{verbatim}
To perform an error analysis based on the Jackknife resampling method (Sec.~\ref{sec:jack})  of the Monte Carlo bins for all observables run the script \texttt{analysis.sh} 
(see Sec.~\ref{sec:analysis}). In case that the parameter \path{N_auto} is set to a finite value the script will also trigger the computation of autocorrelation functions (Sec.~\ref{sec:autocorr}).

\red{[MOVING SECTION ANALYSIS HERE - HAS TO BE SMOOTHED OUT]}


\begin{table}[h]
	\begin{tabular}{@{} l l @{}}\toprule
		Program & Description \\\midrule
		\texttt{cov\_scal.F90}  &  In combination with the script \texttt{analysis.sh}, the bin files with suffix \texttt{\_scal} are read in, \\
		& and  the corresponding files with suffix \texttt{\_scalJ} are produced. They  contain the  result \\
		& of the Jackknife rebinning analysis  (see Sec.~\ref{sec:sampling}).  \\
		\texttt{cov\_eq.F90}    &  In combination with the script \texttt{analysis.sh}, the bin files with suffix \texttt{\_eq} are read in, \\
		& and the corresponding files with suffix  \texttt{\_eqJR}  and  \texttt{\_eqJK}  are produced. They  correspond \\
		& to correlation functions in real and Fourier space, respectively.  \\
		\texttt{cov\_tau.F90}   &  In combination with the script \texttt{analysis.sh}, the bin files  \texttt{X\_tau} are read in, \\
		& and the directories  \texttt{X\_kx\_ky} are produced  for all \texttt{kx} and \texttt{ky} greater or equal to zero. \\
		& Here \texttt{X}  is a place holder from \texttt{Green}, \texttt{SpinXY}, etc   as specified in \texttt{ Alloc\_obs(Ltau)} \\
		& (See section \ref{Alloc_obs_sec}). Each directory contains  a  file    \texttt{g\_kx\_ky}  containing the  \\
		& time displaced correlation function traced over the  orbitals.  It also contains the  \\
		& covariance matrix if \texttt{N\_cov} is set to unity in the parameter file  (see Sec.~\ref{sec:input}). \\
		& Equally, a directory  \texttt{X\_R0}  for the local  time displaced  correlation function is generated.  \\                         
		\texttt{cov\_tau\_ph.F90}            & At compilation time  the file \texttt{cov\_tau\_ph.F90} is generated, and  should be used to compute \\ 
		& particle-hole  imaginary time correlation functions such as Spin and Charge.   Here we use  \\
		&  the fact that these  correlation functions  are symmetric around $\tau = \beta/2$ so that we \\
		&  can define an improved estimator by averaging over $\tau$ and $\beta - \tau$.  
		\\\bottomrule
	\end{tabular}
	\caption{ Overview of analysis programs that are called within the script \texttt{analysis.sh}. \label{table:analysis_programs}}
\end{table}
%
Here we briefly   discuss the analysis programs which read in bins and carry out the error analysis. (See Sec.~\ref{sec:sampling}  for a more detailed discussion.)
Error analysis   is based  on the central limit theorem,  which requires bins to be statistically independent, and also the existence of a well-defined variance  for the observable under consideration. 
The former will be the case if bins are  longer than the autocorrelation time.  The latter has to be checked by the user.  In the parameter file listed in Sec.~\ref{sec:input}, the user  can specify how many initial bins should be omitted (variable \texttt{n\_skip}). 
This  number should be comparable to the autocorrelation time.     
The  rebinning  variable \texttt{N\_rebin} will merge \texttt{N\_rebin}  bins into a single new bin. 
If the autocorrelation time  is smaller than the effective bin size, the error should become independent of the bin size and thereby of the variable \texttt{N\_rebin}.  
Our analysis is based on the Jackknife resampling\cite{efron1981}.
As listed in Table  \ref{table:analysis_programs}  we provide three analysis programs to account for the three observable types. The programs can be found in the directory \texttt{Analysis}  and   are executed by running the  bash shell script 
\texttt{analysis.sh}.
%
\begin{table}[h]
	\begin{tabular}{@{} l l @{}}\toprule
		File & Description \\\midrule
		\texttt{parameters}  &  Contains also variables for the error analysis:\\
		& \texttt{n\_skip}, \texttt{N\_rebin} and \texttt{N\_Cov} (see Sec.~\ref{sec:input}) \\
		\texttt{X\_scal}, \texttt{Y\_eq}, \texttt{Y\_tau} & Monte Carlo bins (see Table \ref{table:output}) \\\bottomrule
	\end{tabular}
	\caption{Standard input files for the error analysis. \label{table:analysis_input}}
\end{table}
%
\begin{table}[h]
	\begin{tabular}{@{} l l l @{}}\toprule
		File & Description \\\midrule
		\texttt{X\_scalJ} & Jackknife mean and error of \texttt{X}, where  \texttt{X} stands for \texttt{Kin, Pot, Part}, and \texttt{Ener}.\\
		\texttt{Y\_eqJR} and \texttt{Y\_eqJK} & Jackknife mean and error of \texttt{Y}, where \texttt{Y} stands for \texttt{Green, SpinZ, SpinXY}, and \texttt{Den}.\\
		& The suffixes \texttt{R} and \texttt{K} refer to real and reciprocal space, respectively.\\
		\texttt{Y\_R0/g\_R0} & Time-resolved and spatially local Jackknife mean and error of \texttt{Y},\\
		& where \texttt{Y} stands for \texttt{Green, SpinZ, SpinXY}, and \texttt{Den}.\\
		\texttt{Y\_kx\_ky/g\_kx\_ky} & Time resolved and $\vec{k}$-dependent Jackknife mean and error of \texttt{Y},\\
		& where \texttt{Y} stands for \texttt{Green, SpinZ, SpinXY}, and \texttt{Den}.\\\bottomrule
	\end{tabular}
	\caption{ Standard output files of the error analysis. \label{table:analysis_output}}
\end{table}
%
In the following, we describe the formatting of the output files mentioned in Table \ref{table:analysis_output}.
\begin{itemize}
	\item For the scalar quantities \texttt{X}, the output files  \texttt{X\_scalJ} have the following formatting:
	\begin{alltt}
		Effective number of bins, and bins:           <N_bin - n_skip>          <N_bin>
		
		OBS :    1      <mean(X)>      <error(X)>
		
		OBS :    2      <mean(sign)>   <error(sign)>
	\end{alltt}
	
	\item For the equal-time correlation functions \texttt{Y}, the formatting of the output files \texttt{Y\_eqJR} and \texttt{Y\_eqJK} follows this structure:
	\begin{alltt}
		do i = 1, N_unit_cell
		<k_x(i)>   <k_y(i)>
		do alpha = 1, N_orbital
		do beta  = 1, N_orbital
		alpha   beta   Re<mean(Y)>   Re<error(Y)>   Im<mean(Y)>   Im<error(Y)>
		enddo
		enddo
		enddo
	\end{alltt}
	where \texttt{Re} and \texttt{Im} refer to the real and imaginary part, respectively.
	
	\item The imaginary-time displaced correlation functions \texttt{Y} are written to the output files \texttt{Y\_R0/g\_R0}, when measured locally in space, 
	and to the output files \texttt{Y\_kx\_ky/g\_kx\_ky} when they are measured $\vec{k}$-resolved.    The first line of the  file prints the number of time slices, 
	the number of bins and the inverse temperature. 
	Both output files have the following formatting:
	\begin{alltt}
		do i = 0, Ltau
		tau(i)   <mean( Tr[Y] )>   <error( Tr[Y])>
		enddo
	\end{alltt}
	where \texttt{Tr} corresponds to the trace over the orbital degrees of freedom.   For particle-hole quantities at finite temperature,  $\tau$ runs from 
	$0$ to $\beta/2$.   In all other cases it runs from $0$ to $\beta$. 
	
	
\end{itemize}



%
%-------------------------------------------------------------------------------------
\subsection{Performance \& Parameter optimization} \label{sec:optimize}
%-------------------------------------------------------------------------------------
%


The finite-temperature, auxiliary-field QMC algorithm is known to be numerically unstable, as discussed in Sec.~\ref{sec:stable}.
The numerical instabilities arise from the imaginary-time propagation, which invariably leads to exponentially small and exponentially large scales.
As shown in Ref.~\cite{Assaad08_rev}, scales can be omitted in the ground state algorithm -- thus rendering it very stable --  but have to be taken into account in the  finite-temperature code.

Numerical stabilization of the code is a delicate procedure that has been pioneered in Ref.~\cite{White89}  for the finite-temperature algorithm and in Refs.~\cite{Sugiyama86,Sorella89} for the zero-temperature projective algorithm.
It is important to be aware of the fragility of the numerical stabilization and that there is no guarantee that it will work for a given model. It is therefore crucial to always check the file \texttt{info}, which, apart from runtime data, contains important information concerning the stability of the code, in particular \texttt{Precision Green}.
If the numerical stabilization fails, one possible measure is to reduce the value of the parameter \texttt{Nwrap} in the parameter file, which will however also impact performance -- see Table.~\ref{table:tips} for further optimization tips. Typical values for the numerical precision ALF can achieve can be found in the examples of Sec.~\ref{sec:ex} (see Sec.~\ref{sec:prec_charge} and \ref{sec:prec_spin}).

In particular, for the stabilization of the involved matrix multiplications we rely on routines from LAPACK. Notice that results are very likely to change
%significantly
depending on the specific implementation of the library used\footnote{The linked library should implement at least the LAPACK-3.4.0 interface.}.
In order to deal with this possibility, we offer a simple baseline which can be used as a quick check as tho whether results depend on the library used for linear algebra routines. Namely, we have included QR-decomposition related routines of the LAPACK-3.6.1 reference implementation from \url{http://www.netlib.org/lapack/}, which you can use by 
%including the switch \texttt{-DQRREF} into the \texttt{STABCONFIGURATION} string in the 
running the script \path{configureHPC.sh}, (described in Sec.~\ref{sec:running}), with the flag \texttt{STAB1} and recompiling ALF\footnote{This flag may trigger compiling issues, in particular, the Intel ifort compiler version 10.1 fails for all optimization levels.}. The stabilization flags available are described in Tab.~\ref{table:configure}

\red{make into table (Tab.~\ref{table:configure}) (comments in caption or text):}

In order to provide further flexibility, we offer various stabilization schemes that can be selected through the appropriate flags when running \texttt{configureHPC.sh}: \texttt{STAB1}, for using the reference stabilization scheme;
\texttt{STAB2}, which sets a stabilization scheme based on the QR decomposition, but not using the LAPACK reference implementation and with additional normalizations;
\texttt{STAB3}, for the newest and fastest stabilization, which separates large and small scales -- it generally works well, but there are models for which it fails;
and \texttt{LOG}, for using log storage for dealing with more extreme scales.

\red{end of to-be-made-into-a-table part}

\red{INCLUDE 'rules of thumb' from notes-ALF.md: ``$<$table$>$"}

